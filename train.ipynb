{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da23af74",
   "metadata": {},
   "source": [
    "üß† Quantization Explained Simply:\n",
    "4-bit quantization = Replace full precision (16/32-bit) weights with compressed 4-bit versions.\n",
    "\n",
    "Used only for inference/fine-tuning, not pretraining.\n",
    "\n",
    "During forward/backward pass, weights are temporarily converted to higher precision (compute_dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"  # NormalFloat4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fc3a3",
   "metadata": {},
   "source": [
    "üîÅ 3. How Does LoRA Work with Quantized Models?\n",
    "LoRA (Low-Rank Adaptation) + Quantization = a powerful combo for efficient fine-tuning:\n",
    "\n",
    "Instead of modifying all model weights, LoRA injects trainable matrices (A, B) into selected layers (like attention).\n",
    "\n",
    "You freeze the base model.\n",
    "\n",
    "You train only the small LoRA adapters, while model runs in quantized (4-bit) mode.\n",
    "\n",
    "This keeps training cost low, memory usage tiny, and supports consumer GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28335d92",
   "metadata": {},
   "source": [
    "</br>\n",
    "üîß What Is Quantization?\n",
    "Quantization is the process of reducing the precision of the numbers used to represent a model‚Äôs weights, activations, or gradients.\n",
    "\n",
    "Normally, models use 32-bit floats (float32) for everything.\n",
    "\n",
    "Quantization replaces them with smaller types, like:\n",
    "\n",
    "float16 (16-bit floating point)\n",
    "\n",
    "int8 (8-bit integer)\n",
    "\n",
    "int4 (4-bit integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17245e",
   "metadata": {},
   "source": [
    "    üß† Why Do We Quantize?\n",
    "Because:\n",
    "\n",
    "Memory is expensive.\n",
    "\n",
    "4-bit numbers take 1/8th the space of 32-bit floats.\n",
    "\n",
    "You can't run 7B+ models on small GPUs unless you compress them.\n",
    "\n",
    "Inference and training speed up because smaller numbers = faster operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dc586",
   "metadata": {},
   "source": [
    "‚öôÔ∏è Real-Life Analogy:\n",
    "Imagine storing prices:\n",
    "\n",
    "32-bit: $32.149732 ‚Üí very precise, but takes space.\n",
    "\n",
    "8-bit: $32.14 ‚Üí enough precision for many tasks.\n",
    "\n",
    "4-bit: $32 ‚Üí rough, but still usable.\n",
    "\n",
    "Quantization keeps the structure, just approximates the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc80e5",
   "metadata": {},
   "source": [
    "üß± 4. Where Is LoRA Applied?\n",
    "Usually applied to:\n",
    "\n",
    "Attention layers:\n",
    "\n",
    "q_proj (query projection)\n",
    "\n",
    "v_proj (value projection)\n",
    "\n",
    "Optionally:\n",
    "\n",
    "k_proj, o_proj, MLP layers\n",
    "\n",
    "Why not all?\n",
    "‚Üí Because attention layers have the most impact with fewer trainable params."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dafaa8",
   "metadata": {},
   "source": [
    "üß© What is PEFT?\n",
    "PEFT stands for:\n",
    "\n",
    "üîß Parameter-Efficient Fine-Tuning\n",
    "\n",
    "It is:\n",
    "\n",
    "A Python library by Hugging Face\n",
    "\n",
    "Built to make fine-tuning large models easier and cheaper\n",
    "\n",
    "Especially helpful for resource-limited environments (like laptops or small GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db394f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
